# bring in our LLAMA_CLOUD_API_KEY
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex
from llama_parse import LlamaParse
from llama_index.core import SimpleDirectoryReader
from llama_index.core.node_parser import SentenceSplitter
import pandas as pd 
import os
from groq import Groq
from llama_index.core.schema import TextNode, NodeRelationship, RelatedNodeInfo
import uuid
from pymongo import MongoClient
import numpy as np
from openai import OpenAI
import pickle
import tiktoken  # Assuming tiktoken is available to count tokens
load_dotenv()

client = OpenAI(api_key = os.environ.get("OPENAI_API_KEY"))

# Function to calculate token count based on a model tokenizer
def count_tokens(text, model="text-embedding-3-small"):
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))
    
def split_text_by_tokens(text, max_tokens, model="text-embedding-3-small"):
    encoding = tiktoken.encoding_for_model(model)
    tokens = encoding.encode(text)
    
    # Return only the first chunk, up to max_tokens
    return encoding.decode(tokens[:max_tokens])


def cosine_similarity(vec1, vec2):
    """Calculate the cosine similarity between two vectors."""
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))


def get_embedding(text, model="text-embedding-3-small"):
    return client.embeddings.create(input = [text], model=model).data[0].embedding

def delete_all_documents(mongo_string, database_name, collection_name):
    # Connect to MongoDB
    client = MongoClient(mongo_string)
    db = client[database_name]
    # Check if the collection exists
    if collection_name in db.list_collection_names():
        # If exists, delete all documents
        collection = db[collection_name]
        delete_result = collection.delete_many({})
        return delete_result.deleted_count
    else:
        # If it doesn't exist, return 0
        return 0


def save_nodes_to_pickle(nodes_full, output_path):
    # Open the file in binary write mode and save the object using pickle
    with open(output_path, 'wb') as f:
        pickle.dump(nodes_full, f)

# Convert numpy types in node.metadata to Python native types
def convert_numpy_types(data):
    if isinstance(data, np.generic):
        return data.item()  # Convert numpy types to Python types
    elif isinstance(data, dict):
        return {key: convert_numpy_types(value) for key, value in data.items()}
    elif isinstance(data, list):
        return [convert_numpy_types(value) for value in data]
    else:
        return data
    

def insert_node_into_mongodb(mongo_string, collection_name, database , node):
    # Create MongoDB connection string
    #uri = f"mongodb+srv://{username}:{password}@{host}/{database}?retryWrites=true&w=majority"
    uri = mongo_string
    # Create a client
    client = MongoClient(uri)
    
    # Access the database and collection
    db = client[database]
    collection = db[collection_name]
    metadata_cleaned = convert_numpy_types(node.metadata)

    node_document = {
        "text": node.text,
        "embedding": node.embedding,  # Save the embedding here
        "metadata": metadata_cleaned
    }
    
    collection.insert_one(node_document)
    return {
        'status': True
    }

def get_llm_openai_response(text_message, model='gpt-4o-mini'):
    """
    Generates a response from OpenAI's language model based on the input message,
    prompt system instructions, and chat history.

    Args:
        message (str): The user's input message.
        prompt_system (str): The system prompt or initial instruction for the model.
        chat_history (list): The list of previous chat messages with roles.
        model_openai (str): The OpenAI model to use (e.g., "gpt-3.5-turbo").

    Returns:
        str: The response generated by the OpenAI model.
    """

    prompt_system = """Eres un experto en resumir. Tu tarea es resumir cualquier texto de la forma más concisa y precisa posible.
                    Tu debes ser capaz de resumir el texto entregando el siguiente formato:
                    1- Nombre del Postulante (Gobernador): 
                    2- Region del Postulante:
                    3- Sintesis de las propuestas:
                    4- Palabras clave:
                    El texto es: {}""".format(text_message)
                    
    # Construct the message list for the OpenAI API call
    message_llm = [{"role": "system", "content": prompt_system}]

    # Initialize OpenAI client
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    # Create a chat completion request
    response = client.chat.completions.create(
        model=model,
        messages=message_llm
    )

    return {
        'status': True, 'message': response.choices[0].message.content
    }


def get_llm_groq_response(text):

    client = Groq(
        api_key=os.environ.get("GROQ_API_KEY"),
    )
    
    # System message instructing the model to act as a summarizer
    system_message = {
        "role": "system",
        "content": """Eres un experto en resumir. Tu tarea es resumir cualquier texto de la forma más concisa y precisa posible.
                      Tu debes ser capaz de resumir el texto entregando el siguiente formato:
                      1- Nombre del Postulante (Gobernador): 
                      2- Region del Postulante:
                      3- Sintesis de las propuestas:
                      4- Palabras clave:"""
    }

    # User message containing the specific text to summarize
    user_message = {
        "role": "user",
        "content": f"Resume el siguiente texto: {text}"
    }

    chat_completion = client.chat.completions.create(
        messages=[system_message, user_message],
        model="llama-3.1-8b-instant",  # Using the appropriate model
    )
    
    return chat_completion.choices[0].message.content


def find_closest_embedding_in_mongodb(username, password, host, database, collection_name, user_embedding):
    # Create MongoDB connection string
    uri = f"mongodb+srv://{username}:{password}@{host}/{database}?retryWrites=true&w=majority"
    
    # Create a client
    client = MongoClient(uri)
    
    # Access the database and collection
    db = client[database]
    collection = db[collection_name]

    # Initialize variables to track the most similar embedding
    max_similarity = -1  # Start with the lowest possible similarity
    closest_document = None
    
    # Retrieve all documents from the collection
    documents = collection.find({}, {"embedding": 1, "text": 1, "metadata": 1})  # Retrieve embeddings, text, and metadata

    return documents

def created_embeddings_database(path_file):

    # Load the environment variables
    mongo_string = os.environ.get("MONGO_STRING_URL")
    collection_name = os.environ.get("COLLECTION_COSMO")
    database = os.environ.get("DATABASE_COSMO")

    # First, delete all records in the collection
    deleted_count = delete_all_documents(mongo_string, database, collection_name)
    
    # set up parser
    parser = LlamaParse(
        result_type="markdown"  # "markdown" and "text" are available,
        
    )
    df = pd.read_excel(path_file)

    nodes_full = []
    for row in range(len(df)):
        print('row')
        print(row)
        unique_id = str(uuid.uuid4())
        file_extractor = {".pdf": parser}
        path_file = df['path_pdf'][row]
        name_file = df['PDF'][row]
        nombre_ = df['Nombre'][row]
        partido_ = df['Partido'][row]
        edad_ = str(df['Edad'][row])
        profesion_ = df['Profesion'][row]
        cargopostulacion_ = df['CargoCandidato'][row]
        pacto_ = df['Pacto'][row]
        region_ = df['Region'][row]
        facebook_ = df['Facebook'][row]
        twitter_ = df['Twitter'][row]
        intasgram_ = df['Intragram'][row]
        website_ = df['Website'][row]
        tiktok_ = df['Tiktok'][row]
        tipo_partido_ = df['Tipo Partido'][row]
        sexo_ = df['Sexo'][row]
        carrera_politica = df['Carrera Politica'][row]
        historial_elecciones = df['Historial Elecciones'][row]
        pdf = df['PDF'][row]
        path_pdf = df['path_pdf'][row]
        documents = SimpleDirectoryReader(input_files=[path_file], file_extractor=file_extractor).load_data()
        splitter = SentenceSplitter(chunk_size=2024,
        chunk_overlap=20)
        nodes = splitter.get_nodes_from_documents(documents)
        
        full_text = ''
        for node in nodes:
            text = node.text
            text_with_metadata = """
            ## METADATA ##
            Nombre Candidato: {} \n
            Partido Canidado: {} \n
            Region Candidato en postulacion: {} \n
            Pacto del candidato en postulacion: {} \n
            ## Texto Chunks ##
            {}
            """.format(nombre_, partido_, region_, pacto_, text)
            
            node.text = text_with_metadata
            embedding_text = get_embedding(text_with_metadata)
            node.embedding = embedding_text
            node.metadata['file_name'] = name_file
            node.metadata['type_name'] = 'pdf'
            node.metadata['nombre'] = nombre_
            node.metadata['partido'] = partido_
            node.metadata['edad'] = edad_
            node.metadata['profesion'] = profesion_
            node.metadata['cargopostulacion'] = cargopostulacion_
            node.metadata['pacto'] = pacto_
            node.metadata['region'] = region_
            node.metadata['facebook'] = facebook_
            node.metadata['twitter'] = twitter_
            node.metadata['intasgram'] = intasgram_
            node.metadata['website'] = website_
            node.metadata['tiktok'] = tiktok_
            node.metadata['tipo_partido'] = tipo_partido_
            node.metadata['sexo'] = sexo_
            node.metadata['carrera_politica'] = carrera_politica
            node.metadata['historial_elecciones'] = historial_elecciones
            full_text += text

        if count_tokens(full_text) > 20000:
            first_chunk = split_text_by_tokens(full_text, 20000)
            resumen_text = get_llm_openai_response(first_chunk)['message']
        else:
            resumen_text = get_llm_openai_response(full_text)['message']

        embedding_text_summary = get_embedding(resumen_text)

        for node in nodes:
            node.metadata['summary'] = resumen_text
            node.metadata['summary_embedding'] = embedding_text_summary

        for node in nodes:
            response_mongo = insert_node_into_mongodb(mongo_string, collection_name, database, node)
            nodes_full.append(node)

    # Example usage:
    output_path = os.path.join("src", "files", "embeddings", "nodes_data.pkl")
    save_nodes_to_pickle(nodes_full, output_path)

    return {
        'status': True,
        'nodes': nodes_full
    }

if __name__ == "__main__":  
    path_file = 'src/files/excel/base.ods'
    response = created_embeddings_database(path_file)
    #print('response')
    #print(response['status'])
